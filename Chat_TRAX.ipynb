{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdkv2UaxqUVC",
        "outputId": "0fd833b0-9f14-457f-caa2-e2460404edf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trax\n",
            "  Downloading trax-1.4.1-py2.py3-none-any.whl (637 kB)\n",
            "\u001b[K     |████████████████████████████████| 637 kB 16.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.21.6)\n",
            "Collecting funcsigs\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 20.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.3.25)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (1.3.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.7.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->trax) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->trax) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.64.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.10.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.9.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.10.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.11.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.6)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.19.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.57.0)\n",
            "Collecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 6.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.27.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.4.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.11.23-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (21.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.50.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 65.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.6.3)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (14.0.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.38.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow, tensorflow-text, funcsigs, trax\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-22.11.23 funcsigs-1.0.2 keras-2.11.0 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0 trax-1.4.1\n"
          ]
        }
      ],
      "source": [
        "! pip install trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjZEm_8CqVkk",
        "outputId": "bde8e72e-273e-4114-985e-8f7771e21adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fHB06tV_qZSo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IaItneT0qcYQ"
      },
      "outputs": [],
      "source": [
        "# filename of the MultiWOZ dialogue dataset\n",
        "DATA_FILE = 'all_conversations.csv'\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "poYHmKcQqsyk"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./TM-3-2020/full-train-dataset.csv')\n",
        "last_id = df['conversation_id'][0]\n",
        "tmp = ''\n",
        "untokenized_data = []\n",
        "for _, row in df.iterrows():\n",
        "    # print(last_id)\n",
        "    if row['conversation_id'] == last_id:\n",
        "        tmp += 'User: ' + str(row['user']) + \\\n",
        "            ' Assistant: ' + str(row['assistant'])\n",
        "    else:\n",
        "        untokenized_data.append(tmp)\n",
        "        tmp = ''\n",
        "    last_id = row['conversation_id']\n",
        "untokenized_data.append(tmp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCmmpjNVvSwS",
        "outputId": "1bd4fb7a-ddc5-46a1-9288-36681ec179a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(untokenized_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-x-KKhXqdpg",
        "outputId": "e521898e-c779-4aba-aafd-fb9542513e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi....am buying a ticket [date.showing] so we go and see a movie at [name.theater] Assistant: No problem. Is there a particular type of movie you’re looking for?User: hhhmmmmm not at all. i dont have any in mind for now Assistant: Sure. I can help with that. Let me listings at A[name.theater]User: sure you can but i want to see the movie at [name.theater] Assistant: Oh, sorry about that. So you're interested in [name.genre] films at [name.theater], right?User: yeah Assistant: OK. I show one [name.genre] movie playing at [name.theater] [name.movie]. Remaining showtimes are [time.showing], [time.showing] and [time.showing]. Does any of those work?User: yeah but [time.showing] will be perfect for me Assistant: Great. And how many tickets?User: myself and two other persons are going to see a movie Assistant: All right. Let me confirm that you'd like [num.tickets] tickets for [name.movie] at [name.theater] [date.showing] at [time.showing]. Is that all correct?User: yeah Assistant: Is it OK to go ahead and purchase these tickets?User: yes you can Assistant: OK. Your tickets are purchased and details for how to proceed have been sent to your email address. Can I help with anything else?User: ok thanks but i dont need anything again Assistant: OK. Enjoy your movie!\n"
          ]
        }
      ],
      "source": [
        "# print the first element to check if it's the same as the one we got before\n",
        "print(untokenized_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfLT4_OnrPUt",
        "outputId": "c065136b-f466-4bf3-8d9e-cdec8a851a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 23761\n",
            "number of conversations in train set: 22573\n",
            "number of conversations in eval set: 1188\n"
          ]
        }
      ],
      "source": [
        "# shuffle the list we generated above\n",
        "random.shuffle(untokenized_data)\n",
        "\n",
        "# define a cutoff (5% of the total length for this assignment)\n",
        "# convert to int because we will use it as a list index\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "# cut_off=1\n",
        "# slice the list. the last elements after the cut_off value will be the eval set. the rest is for training.\n",
        "train_data, eval_data = untokenized_data[:-\n",
        "                                         cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_advYFpyrSrO"
      },
      "outputs": [],
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        d = random.choice(data)\n",
        "        yield (d, d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v23BVZirWUt"
      },
      "outputs": [],
      "source": [
        "# trax allows us to use combinators to generate our data pipeline\n",
        "data_pipeline = trax.data.Serial(\n",
        "    # randomize the stream\n",
        "    trax.data.Shuffle(),\n",
        "\n",
        "    # tokenize the data\n",
        "    trax.data.Tokenize(vocab_file=VOCAB_FILE),\n",
        "\n",
        "    # filter too long sequences\n",
        "    trax.data.FilterByLength(2048),\n",
        "\n",
        "    # bucket by length\n",
        "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
        "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
        "\n",
        "    # add loss weights but do not add it to the padding tokens (i.e. 0)\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5QEeLDLrb6W",
        "outputId": "111c1abc-0210-43a8-af45-c25880a04070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape:  (4, 512)\n",
            "User: Can I purchase movie tickets for [date.showing] please? Assistant: Sure, which movie are you interested in seeing [date.showing]?User: [name.movie] please Assistant: Ok, and which theater would you like to go to?User: [name.theater] Assistant: Ok! What time would work best for you. Available times for [date.showing] are [time.showing] and [time.showing].User: [time.showing] would work Assistant: Great! Lastly, how many tickets are you going to need.User: Just [num.tickets] tickets please, my friends are coming back from college and want to see a movie. Assistant: Perfect. To confirm, you would like to purchase [num.tickets] movie tickets to [name.movie] at [name.theater] at [time.showing].User: Wait, I wanted to go to the [time.showing] movie. Assistant: My apologies, this movie has filled up. Would you like the [time.showing] movie instead?User: Sure that is fine. Assistant: To confirm, you would like to purchase [num.tickets] movie tickets to [name.movie] at [name.theater] at [time.showing].User: Sounds perfect Assistant: One moment while I complete this purchase for you!User: Thanks! Assistant: Complete. Please check your email for barcodes in order to enter the movie purchased. Hope to deal with you again soon!\n"
          ]
        }
      ],
      "source": [
        "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\n",
        "inp, _, _ = next(train_stream)\n",
        "\n",
        "# print the shape. format is (batch size, token length)\n",
        "print(\"input shape: \", inp.shape)\n",
        "\n",
        "# detokenize the first element\n",
        "print(trax.data.detokenize(inp[0], vocab_file=VOCAB_FILE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSMBC5scyMly"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KyC_hJDPrd9p"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: reversible_layer_forward\n",
        "def reversible_layer_forward(x, f, g):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "        x (np.array): an input vector or matrix\n",
        "        f (function): a function which operates on a vector/matrix\n",
        "        g (function): a function which operates on a vector/matrix\n",
        "    Returns: \n",
        "        y (np.array): an output vector or matrix whose form is determined by 'x', f and g\n",
        "    \"\"\"\n",
        "    # split the input vector into two (* along the last axis because it is the depth dimension)\n",
        "    x1, x2 = np.split(x, 2, axis=-1)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # get y1 using equation 3\n",
        "    y1 = x1 + f(x2)\n",
        "\n",
        "    # get y2 using equation 4\n",
        "    y2 = x2 + g(y1)\n",
        "\n",
        "    # concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray\n",
        "    y = np.concatenate([y1, y2], axis=-1)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fX6Eo4uerleS"
      },
      "outputs": [],
      "source": [
        "# UNQ_C3\n",
        "# GRADED FUNCTION: reversible_layer_reverse\n",
        "def reversible_layer_reverse(y, f, g):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "        y (np.array): an input vector or matrix\n",
        "        f (function): a function which operates on a vector/matrix of the form of 'y'\n",
        "        g (function): a function which operates on a vector/matrix of the form of 'y'\n",
        "    Returns: \n",
        "        y (np.array): an output vector or matrix whose form is determined by 'y', f and g\n",
        "    \"\"\"\n",
        "\n",
        "    # split the input vector into two (* along the last axis because it is the depth dimension)\n",
        "    y1, y2 = np.split(y, 2, axis=-1)\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # compute x2 using equation 5\n",
        "    x2 = y2 - g(y1)\n",
        "\n",
        "    # compute x1 using equation 6\n",
        "    x1 = y1 - f(x2)\n",
        "\n",
        "    # concatenate x1 and x2 along the depth dimension\n",
        "    x = np.concatenate([x1, x2], axis=-1)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ClXQDI4_rzv3"
      },
      "outputs": [],
      "source": [
        "# UNQ_C4\n",
        "# GRADED FUNCTION\n",
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # initialize an instance of Trax's ReformerLM class\n",
        "    model = tl.Serial(\n",
        "        trax.models.reformer.ReformerLM(\n",
        "            # set vocab size\n",
        "            vocab_size=vocab_size,\n",
        "            # set number of layers\n",
        "            n_layers=n_layers,\n",
        "            # set mode\n",
        "            mode=mode,\n",
        "            # set attention type\n",
        "            attention_type=attention_type\n",
        "        ), tl.LogSoftmax()\n",
        "    )\n",
        "    ### END CODE HERE ###\n",
        "    return model  # tl.Serial(model, tl.LogSoftmax(),)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHrIAIg040Wt",
        "outputId": "08bb9578-a733-4a51-861f-5d2fac079107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    Serial[\n",
            "      ShiftRight(1)\n",
            "    ]\n",
            "    Embedding_train_512\n",
            "    Dropout\n",
            "    Serial[\n",
            "      PositionalEncoding\n",
            "    ]\n",
            "    Dup_out2\n",
            "    ReversibleSerial_in2_out2[\n",
            "      ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "        ]\n",
            "        SelfAttention\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "      ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "          Dense_2048\n",
            "          Dropout\n",
            "          Serial[\n",
            "            FastGelu\n",
            "          ]\n",
            "          Dense_512\n",
            "          Dropout\n",
            "        ]\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "      ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "        ]\n",
            "        SelfAttention\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "      ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "        Serial[\n",
            "          LayerNorm\n",
            "          Dense_2048\n",
            "          Dropout\n",
            "          Serial[\n",
            "            FastGelu\n",
            "          ]\n",
            "          Dense_512\n",
            "          Dropout\n",
            "        ]\n",
            "      ]\n",
            "      ReversibleSwap_in2_out2\n",
            "    ]\n",
            "    Concatenate_in2\n",
            "    LayerNorm\n",
            "    Dropout\n",
            "    Serial[\n",
            "      Dense_train\n",
            "    ]\n",
            "  ]\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# display the model\n",
        "temp_model = ReformerLM('train')\n",
        "print(str(temp_model))\n",
        "\n",
        "# free memory\n",
        "#del temp_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BapHJ8oAr9Mo"
      },
      "outputs": [],
      "source": [
        "# UNQ_C5\n",
        "# GRADED FUNCTION: train_model\n",
        "def training_loop(ReformerLM, train_gen, eval_gen, output_dir=\"./models/model_\"):\n",
        "\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
        "        n_warmup_steps=1000, max_value=0.01)\n",
        "\n",
        "    train_task = training.TrainTask(\n",
        "\n",
        "        labeled_data=train_gen,\n",
        "\n",
        "        loss_layer=tl.CrossEntropyLoss(),\n",
        "\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "\n",
        "        lr_schedule=lr_schedule,\n",
        "\n",
        "        n_steps_per_checkpoint=10\n",
        "    )\n",
        "    eval_task = training.EvalTask(\n",
        "\n",
        "        labeled_data=eval_gen,\n",
        "\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
        "\n",
        "    loop=training.Loop(ReformerLM(mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task], output_dir='.models/model_full_data_1')\n",
        "    return loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "nhJvCK65sENo",
        "outputId": "a32e18d2-08b1-4e94-c4b8-9ed79faadaea"
      },
      "outputs": [],
      "source": [
        "# we will now test your function\n",
        "loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "loop.run(500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I64SE4hksMsC"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, vocab_file):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file))[0]\n",
        "\n",
        "\n",
        "def detokenize(tokens, vocab_file):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4uEPU1BXsOE8"
      },
      "outputs": [],
      "source": [
        "# UNQ_C6\n",
        "# GRADED FUNCTION\n",
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, temperature, tokenize=tokenize):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Create input tokens using the the tokenize function\n",
        "    input_tokens = tokenize(start_sentence, vocab_file=vocab_file)\n",
        "\n",
        "    # Add batch dimension to array. Convert from (n,) to (1, n)\n",
        "    input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
        "\n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream(\n",
        "        # model\n",
        "        ReformerLM,\n",
        "        # inputs will be the tokens with batch dimension\n",
        "        inputs=input_tokens_with_batch,\n",
        "        # temperature\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return output_gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g81lcLDPzedL"
      },
      "outputs": [],
      "source": [
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120  # max length for predictions\n",
        "    kwargs['predict_drop_len'] = 120  # never drop old stuff\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "\n",
        "model = ReformerLM(\n",
        "    # vocab_size=33000,\n",
        "    # n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q1QQgDA32ITX"
      },
      "outputs": [],
      "source": [
        "model.init_from_file('./models/model_full_data/model.pkl.gz',\n",
        "                     weights_only=True,\n",
        "                     input_signature=shape11)\n",
        "\n",
        "STARTING_STATE = model.state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "dyUY1WIZsXT-"
      },
      "outputs": [],
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file,  max_len, temperature):\n",
        "    \n",
        "    delimiter_1 = 'User: '\n",
        "    delimiter_2 = 'Assistant: '\n",
        "    \n",
        "    sentence = ''\n",
        "    \n",
        "    counter = 0\n",
        "    \n",
        "    result = [tokenize(': ', vocab_file=vocab_file)]\n",
        "    \n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    output = ReformerLM_output_gen(\n",
        "        ReformerLM, start_sentence, vocab_file=VOCAB_FILE, temperature=temperature)\n",
        "    \n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    for o in output:\n",
        "\n",
        "        result.append(o)\n",
        "\n",
        "        sentence = detokenize(np.concatenate(\n",
        "            result, axis=0), vocab_file=VOCAB_FILE)        \n",
        "\n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        if counter > max_len:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgcDDHKdsdfM",
        "outputId": "2564216a-abb7-4352-8015-05c8ccb0ac20"
      },
      "outputs": [],
      "source": [
        "sample_sentence = 'User: Hi, I want to order movie tickets for a movie for tomorrow Assistant: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE,\n",
        "                  start_sentence=sample_sentence, vocab_file=VOCAB_FILE, max_len=120, temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOkWal--x6Wn",
        "outputId": "30487e81-15d6-48e8-f2d7-0b83ad63ddcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, that is right.\n",
            "User: : \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = 'Yes, that is right. '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE,\n",
        "                  start_sentence=sample_sentence, vocab_file=VOCAB_FILE, max_len=30, temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIkkNSv1D2Ct",
        "outputId": "abf608cc-8da4-4cec-8df3-a065dac93026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: I want to buy [num.tickets] movie tickets for [name.movie]\n",
            "Assistant: : I have [name.\n"
          ]
        }
      ],
      "source": [
        "sample_sentence = 'User: I want to buy [num.tickets] movie tickets for [name.movie] Assistant: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE,\n",
        "                  start_sentence=sample_sentence, vocab_file=VOCAB_FILE, max_len=120, temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXfL8LUUO8yN",
        "outputId": "bb586f8b-4413-4bb2-b566-7780e90b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: I am interested in seeing the movie [name.movie].\n"
          ]
        }
      ],
      "source": [
        "sample_sentence = 'User: I am interested in seeing the movie [name.movie].  Assistant: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE,\n",
        "                  start_sentence=sample_sentence, vocab_file=VOCAB_FILE, max_len=120, temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63-96WT9eipb",
        "outputId": "a2ae0d37-9973-4529-82a8-7999d336cb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I would like to buy [num.tickets] tickets to see [name.movie].\n",
            "Assistant: : I have [name.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sample_sentence = \"I would like to buy [num.tickets] tickets to see [name.movie].\"\n",
        "results = generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence +' Assistant: ', vocab_file=VOCAB_FILE, max_len=80, temperature=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml1MHsG1wo4w",
        "outputId": "757ebba7-7d8d-4786-d66e-d2044b42b460"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.275"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from Levenshtein import ratio\n",
        "ratio(['You would like to purchase [num.tickets] ticket to see [name.movie]'], [\n",
        "      'I have [name.theater]  [name.theater]  [name.movie]  [name.theater]  [name.'], processor=lambda s: s[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qBstmRRyFH3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "04b10dac463e5f87143e9fe1a4798e8f4ac3c349c3129d4e52be6a5df56531f8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
